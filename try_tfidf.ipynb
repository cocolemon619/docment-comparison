{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install janome\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall python-docx\n",
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_idf import doc1,doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc = defaultdict(dict)\n",
    "Doc[0][\"doc\"] = str(doc1)\n",
    "Doc[1][\"doc\"] = str(doc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Doc[0][\"doc\"])\n",
    "print(Doc[1][\"doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Doc.keys():\n",
    "    tmp = [t.replace(' ','').lower() for t in Doc[i][\"doc\"]]\n",
    "    Doc[i][\"doc\"] = \"\".join(tmp)\n",
    "print(\"len(Doc[0][doc]): {}\".format(len(Doc[0][\"doc\"])))\n",
    "print(Doc[0][\"doc\"])\n",
    "print(\"len(Doc[1][doc]): {}\".format(len(Doc[1][\"doc\"])))\n",
    "print(Doc[1][\"doc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.tokenfilter import POSStopFilter\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "token_filters = [POSStopFilter(['記号','助詞','助動詞','動詞'])]\n",
    "a = Analyzer(tokenizer=tokenizer, token_filters=token_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokens = a.analyze(Doc[0][\"doc\"])\n",
    "for t in test_tokens:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in Doc:\n",
    "    texts_flat = list(tokenizer.tokenize(Doc[i][\"doc\"], wakati=True))\n",
    "    tokens = a.analyze(' '.join(texts_flat))  # `a.analyze` が文字列を受け取る場合\n",
    "    Doc[i][\"wakati\"] = ' '.join([t.surface for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Doc[0][wakati]: {}\".format(Doc[0][\"wakati\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform([Doc[i][\"wakati\"] for i in Doc.keys()])\n",
    "for i, bow in enumerate(X.toarray()):\n",
    "    Doc[i][\"bow\"] = bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS = vectorizer.get_feature_names_out()\n",
    "print(WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "for w_idx , count in enumerate(Doc[0][\"bow\"]):\n",
    "    if count >= n:\n",
    "        print(\" - {}\\t{}:Words[{}]\".format(count, WORDS[w_idx],w_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tf(b_idx, w_idx):\n",
    "    \"\"\"b_idx 番目の WORD[w_idx] の TF値を算出する\"\"\"\n",
    "    # WORD[w_idx] の出現回数の和\n",
    "    word_count = Doc[b_idx][\"bow\"][w_idx]\n",
    "    if word_count == 0:\n",
    "        return 0.0\n",
    "    # 全単語の出現回数の和\n",
    "    sum_of_words = sum(Doc[b_idx][\"bow\"])\n",
    "    # TF値計計算\n",
    "    return word_count/float(sum_of_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "sample_tfs = [calc_tf(index, w_idx) for w_idx, word in enumerate(WORDS)]\n",
    "tfs_sorted = sorted(enumerate(sample_tfs), key=lambda x:x[1], reverse=True)\n",
    "for i, tf in tfs_sorted[:20]:\n",
    "    print(\"{}\\t{}\".format(WORDS[i], round(tf, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calc_idf(w_idx):\n",
    "    \"\"\"WORD[w_idx] の IDF値を算出する\"\"\"\n",
    "    # 総文書数\n",
    "    N = len(Doc.keys())\n",
    "    print(f\"N:{N}\")\n",
    "    # 単語 word が出現する文書数 df を計算\n",
    "    df = len([i for i in Doc.keys() if Doc[i][\"bow\"][w_idx] > 0])\n",
    "    # idf を計算\n",
    "    return math.log2(N/float(df + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDFS = [calc_idf(w_idx) for w_idx, word in enumerate(WORDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(IDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs_sorted  = sorted(enumerate(IDFS), key=lambda x:x[1], reverse=True)\n",
    "print(\"# IDF values\")\n",
    "for w_idx, idf in idfs_sorted[:10]:\n",
    "    print(\"{}\\t{}\".format(WORDS[w_idx], round(idf, 4)))\n",
    "print(\"︙\")\n",
    "for w_idx, idf in idfs_sorted[-10:]:\n",
    "    print(\"{}\\t{}\".format(WORDS[w_idx], round(idf, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tfidf(b_idx, w_idx):\n",
    "    \"\"\"b_idx 番目の WORD[w_idx] の TF-IDF値を算出する\"\"\"\n",
    "    return calc_tf(b_idx, w_idx) * calc_idf(w_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "sample_tfs = [calc_tfidf(index, w_idx) for w_idx, word in enumerate(WORDS)]\n",
    "tfs_sorted = sorted(enumerate(sample_tfs), key=lambda x:x[1], reverse=True)\n",
    "for i, tf in tfs_sorted[:20]:\n",
    "    print(\"{}\\t{}\".format(WORDS[i], round(tf, 4)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
